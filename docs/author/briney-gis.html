<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>Savage, MN GIS and Aerial Solutions - Briney GIS</title>
        <link rel="stylesheet" href="../theme/css/main.css" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="../">Savage, MN GIS and Aerial Solutions</a></h1>
                <nav><ul>
                    <li><a href="../pages/home.html">Welcome</a></li>
                    <li><a href="../pages/about-me.html">About Me</a></li>
                    <li><a href="../category/drones.html">Drones</a></li>
                    <li><a href="../category/gis.html">GIS</a></li>
                    <li><a href="../category/python-examples.html">Python Examples</a></li>
                    <li><a href="../category/tornado-analysis.html">Tornado Analysis</a></li>
                    <li><a href="../category/tornados.html">Tornados</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="../articles/disaster-mapping.html">Disaster Mapping in Loubiere, Dominica</a></h1>
<footer class="post-info">
        <abbr class="published" title="2025-02-19T00:00:00-06:00">
                Published: Wed 19 February 2025
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="../author/briney-gis.html">Briney GIS</a>
        </address>
<p>In <a href="../category/gis.html">GIS</a>.</p>
<p>tags: <a href="../tag/disaster.html">Disaster</a> <a href="../tag/hurricanes.html">Hurricanes</a> <a href="../tag/gis.html">GIS</a> <a href="../tag/drones.html">Drones</a> </p>
</footer><!-- /.post-info --><p><img src= "../images/Dominica_map.png" alt ="Map of Dominica" style= " width 150px; height: 150px;"></p>
<h2><strong>Disaster in Dominica</strong></h2>
<p>On September, 18th 2017 Hurricane Maria made landfall on the small caribbean island of Dominica as a Category 5 with 167 mph winds. The storm was rare in the speed at which it went from Category 3 to 5 in less than 24 hours -- giving islands residents around 2 hours of notice. The storm struck at night and by the next morning it had decimated the island's infrastructure, buildings and forests. 100% of island's 73,800 residents were affected. The entire island lost power. 90% of buildings were damaged due to intense rainfall, flooding, and landslides. The lush green forests were mostly damaged, including trees being uprooted, snapped or shredded making its landscapes barely recognizable <a href="https://youvis.it/73Azee">Dominica Images post hurricane Maria</a>.</p>
<p>Damage assessment of the ~30,000 buildings on this 750 sq. km island nation took 3 months to complete. 140 public servants and volunteers used tablets and other traditional ground-based structural data collection methods. These assessors categorized and documented the degree of damage by the level of roof damage. They found ~75% of buildings to have 25% or more of the roof damaged, including  ~20% that were totally destroyed.   </p>
<p>The scale and intensity of such a disaster  requires a more efficient and safe means of damage assessment. Ultimately, prolonged damaged assessment times delay response times and repairs. The hazards of navigating a damaged landscape and challenging terrain may also lead to inconsistent documentation preventing appropriate resource allocation and planning.</p>
<p>The goal of this blog post is to provide a high-level tutorial using Esri's deep learning model tools so that you might be able to perform your own building damage assessments in the wake of a disaster. We will start with a UAV image dataset that has been processed into a 2D orthophoto of Dominica post-Maria and load that image into ArcGIS Pro. You can find that image here if you want follow the walkthrough <a href="../GIS/InputData/Dominica2017PostMaria.tif">Download the image data</a> </p>
<h3>User Requirements / (My local system environment)</h3>
<ul>
<li>ArcGIS Pro (3.3)</li>
<li>ArcGIS Pro Image Analyst (Yes)</li>
<li>Deep Learning Libraries  <a href="https://github.com/esri/deep-learning-frameworks?tab=readme-ov-file">Download here</a> (Deep Learning Libraries Installer for ArcGIS Pro 3.3)</li>
<li>NVIDIA GPU (NVIDIA GEForce RTX 1750 GPU)</li>
<li>Computer  (Dell G5 i7)</li>
</ul>
<h3><strong><em>What are Deep Learning Models?</em></strong></h3>
<p>Deep Learning is a sub-discipline in the larger field of Artificial Intelligence (AI) that studies various ways to have computers perform tasks requiring human intelligence. It is a type of Machine Learning (ML), where the computer program is trained with examples to learn to identify patterns. In contrast to ML, Deep Learning (DL) has a higher capacity to learn complex patterns. This comes from the way in which DL models use algorithms called neural networks that are structured in a layered interconnected network of nodes. Each node performs calculations on inputs from the prior layer of nodes, and each layer of nodes builds on the learned patterns in the previous layer, allowing the network to learn increasingly more complex patterns. If we think of an image detection DL network trained to recognize cave entrances, a simple arch pattern could be learned in the first layer. As DL model builds on this layer it goes into deeper layers with each resolving more complex patterns. The final layer has learned a 'deep' representation of the data that has all the features needed to detect a cave entrance. </p>
<h2><strong><em>Extract Building footprints in ArcGIS Pro</em></strong></h2>
<p>We start with Esri's pre-trained DL models in ArcGIS Pro to extract building footprints in our disaster site imagery. Pre-trained DL models are ready-to-use DL models built for a specific task, such as land-cover classification or detection objects. By using these pt-DL models we greatly reduce the burden of manually digitizing features and the time it takes to train, and iteratively test it from scratch to achieve high accuracy. This greatly speeds up the workflow and these 'out-of-the-box' pt-DL models may do very well. In our case, the relevant pt-DL model is the Building Footprint Extraction model available on Esri's ArcGIS Living Atlas (<a href="https://www.arcgis.com/home/item.html?id=a6857359a1cd44839781a4f113cd5934">Download here</a>). Building footprint extraction will create features that identify the buildings in the study area, which is critical to assess which ones are damaged. Download the model and move it to a folder that will hold all your pt-DL models.</p>
<p>This pt-DL models work well when the your imagery is similar to the those used to train the model. So, it is key to understand what type of image the model is expecting. This enables us to know whether and how to modify our own input imagery --if needed. <img alt="DL Model Expected Input" src="../images/Esri_ptModel_input_expectations.png"></p>
<p>The pt-DL model expects input in the form of 8-bit imagery with 3 bands. Additionally, the input should have a resolution of 10-40 cm, meaning that each pixel in the image represents area of 10-40cm by 10-40cm on the ground. The range rather than single value suggests that the model was trained on imagery of different resolutions (i.e., From 10-40cm). The output will be a newly created feature class with polygons around buildings. Not mentioned here is the expectation that you have nadir or birds-eye-view image. This is in contrast, to angled imagery from street view camera which will not work well. The expected geography is US but it can work well elsewhere. Model Architecture is Mask RCNN. The average precision shown here is ~72% and elsewhere the same is shown as 79%, meaning ~70-80% of the objects the model detects as buildings are in fact buildings and 20-30% are not actually buildings. </p>
<p>The key here is that each DL model has its own metadata or image formatting and labeling requirements. So, we note the model name and use that to select a compatible metadata format later in the tutorial. To have the optimal building detection result, first we need to verify and/or modify our input image such that it aligns with these formatting requirements. </p>
<h3><strong><em>Matching your Image Properties to the Model</em></strong></h3>
<p>Drag and drop the .tif imagery onto the map in ArcGIS Pro. Go to the contents and navigate to the image properties. Note the number bands is 4. For RGB UAV images like the DJI drone used in this case, the 4th band is the alpha channel that represents the transparency level of each pixel. A pixel with an RGBA value of 0 is fully transparent, whereas one with a value of 255 is fully visible. So, if the pixel is a value of 0 it will be invisible and instead reveal the layer beneath it. Try clicking on various places in the image and you will notice that all pixels have a value of 255 for the RGBA band, meaning that all pixels are fully visible. We need to remove this additional band information from the image. <img src= "../images/4bandRGBA_Imagery.png" alt ="4 Band RGBA Imagery" style= " width 150px; height: 150px;"></p>
<p>The other difference is the input image cell size of ~4.7 cm which is different than what the model expects. In our case, we will not modify resolution as it is not drastically different from 10 cm. But this input cell size is important to note down for later in the process.</p>
<p>The typical way to address the format issues is use ArcGIS Pro's Raster Function Extract Bands on your input TIF image. In the geoprocessing window you would input the values of the bands your in your image and set the Missing Band Action parameter to fail to ensure you correctly extracted 3 bands. After this step, you would then need to save that result because raster function outputs are temporary. To do that you would right-click on the new raster in your contents pane, select Data, and use Export Raster to save it permanently as TIF. However, here I want to advocate for automating this band extraction process and other aspect of the workflow using python scripting and the arcpy library. </p>
<h4><strong>What is Arcpy?</strong></h4>
<p>Arcpy is a Python library for automating GIS tasks in Arc GIS Pro. It allows you to perform geoprocessing, analysis , data management and mapping automation using python.
Here is a script that would accomplish both these tasks : extracting bands and saving a properly formatted new TIF. This script will function as a new tool allowing you to interact with it inside ArcGIS Pro just like other geoprocessing tools. This means you can re-use it, change the inputs, file names and folders.</p>
<div class="highlight"><pre><span></span><code><span class="c1">### This script automates Raster conversion and formatting for generating 8-bit 3-band TIF Files</span>

<span class="kn">import</span> <span class="nn">arcpy</span>
<span class="kn">import</span> <span class="nn">os</span> 


<span class="c1">###================== Set up folders and settings </span>

<span class="c1"># set input raster, output folder and raster name</span>
<span class="n">input_raster</span> <span class="o">=</span> <span class="n">arcpy</span><span class="o">.</span><span class="n">GetParameterAsText</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># get input raster path</span>
<span class="n">output_folder</span> <span class="o">=</span> <span class="n">arcpy</span><span class="o">.</span><span class="n">GetParameterAsText</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># get output folder</span>
<span class="n">output_raster_name</span> <span class="o">=</span> <span class="n">arcpy</span><span class="o">.</span><span class="n">GetParameterAsText</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># get the raster name</span>

<span class="c1"># combine to make output file path</span>
<span class="n">output_raster</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_folder</span><span class="p">,</span> <span class="n">output_raster_name</span><span class="o">+</span> <span class="s2">&quot;.tif&quot;</span><span class="p">)</span>

<span class="c1"># Set environment workspace and enable output overwrite option</span>
<span class="n">arcpy</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">workspace</span> <span class="o">=</span> <span class="n">output_folder</span>
<span class="n">arcpy</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">overwriteOutput</span><span class="o">=</span><span class="kc">True</span>



<span class="c1">###================== Extract bands and save the raster as 8-bit TIF  </span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">extracted_raster</span> <span class="o">=</span> <span class="n">arcpy</span><span class="o">.</span><span class="n">ia</span><span class="o">.</span><span class="n">ExtractBand</span><span class="p">(</span><span class="n">input_raster</span><span class="p">,[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span><span class="n">missing_band_action</span><span class="o">=</span><span class="s1">&#39;Fail&#39;</span> <span class="p">)</span>

    <span class="c1"># save the output raster temporarily</span>
    <span class="n">temp_raster</span> <span class="o">=</span> <span class="s2">&quot;in_memory</span><span class="se">\\</span><span class="s2">temp_raster&quot;</span>
    <span class="n">extracted_raster</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">temp_raster</span><span class="p">)</span>

    <span class="c1"># resample to 0.1 meter per pixel</span>
    <span class="n">resampled_raster</span><span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_folder</span><span class="p">,</span><span class="s2">&quot;resampled_temp.tif&quot;</span><span class="p">)</span>
    <span class="n">arcpy</span><span class="o">.</span><span class="n">management</span><span class="o">.</span><span class="n">Resample</span><span class="p">(</span><span class="n">temp_raster</span><span class="p">,</span><span class="n">resampled_raster</span><span class="p">,</span><span class="n">cell_size</span><span class="o">=</span><span class="s2">&quot;0.1&quot;</span><span class="p">,</span> <span class="n">resampling_type</span><span class="o">=</span><span class="s2">&quot;BILINEAR&quot;</span><span class="p">)</span>


    <span class="c1"># copy to specify the no data value converting 0,0,0 pixels to NoData rather than 0,0,0 pixel appearing as black after removing the alpha channel</span>
    <span class="c1"># convert to 8-bit unsigned with proper scaling if not already done.</span>
    <span class="n">arcpy</span><span class="o">.</span><span class="n">management</span><span class="o">.</span><span class="n">CopyRaster</span><span class="p">(</span><span class="n">resampled_raster</span><span class="p">,</span><span class="n">output_raster</span><span class="p">,</span><span class="n">pixel_type</span><span class="o">=</span><span class="s2">&quot;8_BIT_UNSIGNED&quot;</span><span class="p">,</span> <span class="n">scale_pixel_value</span><span class="o">=</span><span class="s2">&quot;ScalePixelValue&quot;</span><span class="p">,</span><span class="n">nodata_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;TIFF&quot;</span><span class="p">)</span> <span class="c1"># no data value is 0, change if your no Data value is different</span>

    <span class="c1"># Display the output in ArcGIS</span>
    <span class="n">arcpy</span><span class="o">.</span><span class="n">SetParameterAsText</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">output_raster</span><span class="p">)</span>

    <span class="c1"># add success message (green text) to show it completes</span>
    <span class="n">arcpy</span><span class="o">.</span><span class="n">AddMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saved resampled raster with NoData value applied at </span><span class="si">{</span><span class="n">output_raster</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="n">arcpy</span><span class="o">.</span><span class="n">AddError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error processing raster: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1"># critical errors that cause failure (red text)</span>
    <span class="n">arcpy</span><span class="o">.</span><span class="n">AddMessage</span><span class="p">(</span><span class="n">arcpy</span><span class="o">.</span><span class="n">GetMessages</span><span class="p">())</span> <span class="c1"># show all messages, missing inputs, updates, success messages</span>
</code></pre></div>

<p>After creating this script and saving it as a .py file in your project folder you would need to load it into your toolbox in ArcGIS Pro and set up parameter labels and datatypes (See here for details). 
Once that is done the tool is ready to use. To use the python script tool, you need to double-click on the python tool icon, enter the required parameters and click run, as you would any geoprocessing tool. 
If you would like to try it out here is the zipfile with toolbox, python script and a readMe for instructions <a href="../GIS/Tools/BuildingDamage_PreprocessingToolBox.zip">Download ArcGIS toolbox</a>. </p>
<h3><strong><em> Transfer Learning</em></strong></h3>
<p>Transfer learning is a ML technique where knowledge from one task is applied to another related task to boost performance. 
In our case, the pt-DL model's original task was detecting buildings using 10-40cm imagery in the US. 
Now we want to apply it to 5 cm imagery in Dominica. Transfer learning allows us to reduce the samples needed to train a DL model and speeds up the process.</p>
<h4>Label Building footprints</h4>
<p>Now we label the buildings in the image to create training samples for the building footprint model. For this we go the the imagery Tab in ArcGIS Pro and select DeepLearning Tools.
<img src= "../images/Create_Samples_DL_Model_map.png" alt ="Image of the tool to create samples for training a Deep Learning model" style= " width 150px; height: 150px;">
This tool will allow you to create polygon training samples and export them in the format required by the model.
When prompted, select 'Label Using Existing imagery Layer' and then select your TIF as the input. Give the training samples a name and then use the polygon tool to manually label your samples.
As these building footprint samples will form the basis for our damage classification and we are using less than ideal <strong>post</strong>-damage imagery, we will label all ~300 buildings.
<img src= "../images/LabeledBuildings.png" alt ="Labeled building footprint samples for training a Deep Learning model" style= " width 150px; height: 150px;">
<strong><em>Tip</em></strong> : Be sure to be as accurate as possible when creating the labels. Also, if leaving some portion of the image unlabeled, use only the labeled extent for creating the samples
Create a Geodatabase for your project and store the training samples in it. Add the training sample feature class to the map. 
Open the attribute tables, add a new field called Class with datatype Short (integers) and populate all entries with a value of 1. 
In our case, there is only one category of object or class building footprints. However, there could be multiple classes created, if interested in detecting distinct objects (cars, buses, lamp posts). 
The pt-DL model expects one class to identify all objects of the same class. The Class field will tell the model that all objects with a value of 1 are the in the same category i.e., building footprints.</p>
<p><strong>Note:</strong> If you decided to use the above provided custom toolbox for raster reformatting, it also has the option to use a Model Builder workflow that combines reformatting and creating a new feature class. It will both create the reformatted raster and create a new feature class ready to be populated with your manually digitized buildings. In that case, to digitize training samples for pt-DL model, you would not use the Deep Learning tools in the imagery tab, instead run the Model Builder workflow, and load and select your new polygon feature class. Select the Create Features tool in the Edit tab, begin by digitizing the first building and populating your first digitized feature with a class value and class name as 1. This way all following features will have the same values. After creating the samples, you would use the same parameters to export them as shown above but using the geoprocessing tool Export Training Data for Deep Learning Tool.</p>
<h4>Optionally, Clip your image to the Labeled Samples</h4>
<p>If your labeled samples do not span the entire image or you have objects without labels, use the Clip Raster Tool with the TIF as input and extent set using your labeled training samples</p>
<h3><strong><em> Transfer Learning</em></strong></h3>
<h4>Export the Training Data</h4>
<p>Not that the samples are created, we export the samples in the correct size and format for our particular DL model. Select the Export training Data tab.
<img src= "../images/ExportingTrainingData.png" alt ="Parameters for exporting samples for training a Deep Learning model" style= " width 400px; height: 400px;"></p>
<p>DL models in ArcGIS Pro train on images after breaking them into parts called image chips. These image chips are what the model will see and each one of those chips is has two associated files: an image file and corresponding label file. The label shows the model where the objects are in the image.</p>
<p>An important parameter to consider is the size of the chips entered in the Tile size parameter. This tile should be large enough to fully enclose multiple objects that you want to detect i.e., multiple buildings.
The tile size is measured in pixels. To figure out what your tile size is, use the measure tool to measure the length of objects in your input image. Note down the length in meters of the longest object you measured. </p>
<p>Use the following formula to find the pixel size:</p>
<p><code>Building Length in Pixels = Building Length in Meters / Image resolution in Meters Per Pixel</code></p>
<p><code>EXAMPLE: longest building length = 36 , image resolution = 0.047 meters per pixel</code>
<code>38 m / 0.1 m /px  = 380 pixels</code></p>
<!DOCTYPE html>
<p><html>
<head>
    <title>Dynamic Tile Size Visualization</title>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/brython@3.9.5/brython.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.4.0/p5.js"></script>
</head></p>
<body onload="brython()">
    <h2>Tile Size Simulator</h2>

    <label for="maxBuildingSize">Max Building Size (meters):</label>
    <input type="number" id="maxBuildingSize" value="36" step="1">

    <label for="imageResolution">Image Resolution (meters per pixel):</label>
    <input type="number" id="imageResolution" value="0.047" step="0.001">

    <button onclick="updateTileSize()">Compute Best Tile Size</button>

    <p id="tileLabel">Recommended Tile Size: 512 x 512 pixels</p>

    <div id="canvas-holder"></div>

    <script type="text/python">
        from browser import document, window

        def update_tile_size(event=None):
            """ Compute the best tile size dynamically and call JS to update visualization """
            max_building_meters = float(document["maxBuildingSize"].value)
            image_resolution = float(document["imageResolution"].value)

            # Convert max building size from meters to pixels
            max_building_pixels = max_building_meters / image_resolution

            # Select the nearest valid deep learning tile size
            valid_tile_sizes = [256, 512, 1024, 2048]
            optimal_tile_size = min(valid_tile_sizes, key=lambda x: abs(x - max_building_pixels))

            # Update tile label in HTML
            document["tileLabel"].text = f"Recommended Tile Size: {optimal_tile_size} x {optimal_tile_size} pixels"

            # Call JS function to update the grid visualization
            window.redraw_grid(optimal_tile_size)

        # Bind function to the button click event
        document["maxBuildingSize"].bind("input", update_tile_size)
        document["imageResolution"].bind("input", update_tile_size)
    </script>

    <script>
        let img;
        let tileSize = 512; // Default starting value

        function preload() {
            img = loadImage("/images/Dominica2017_RGB_Extracted_resized.png");  // Load the raster image
        }

        function setup() {
            let cnv = createCanvas(1200, 800);  // Match canvas to image dimensions
            cnv.parent("canvas-holder");
            noLoop();
        }

        function draw() {
            image(img, 0, 0, width, height);  // Ensure the image covers the entire canvas

            let cols = width / tileSize;
            let rows = height / tileSize;

            stroke(255, 0, 0);
            noFill();

            for (let i = 0; i < cols; i++) {
                for (let j = 0; j < rows; j++) {
                    rect(i * tileSize, j * tileSize, tileSize, tileSize);
                }
            }
        }

        function redraw_grid(newTileSize) {
            tileSize = newTileSize;
            redraw();
        }

        function updateTileSize() {
            document.dispatchEvent(new Event("input")); // Trigger Brython event
        }
    </script>
</body>
<p></html></p>
<p>Although, DL models have standard image sizes they work best with, which are 128, 256, 512, 1024 pixels. There is evidence from the Esri developer community that non-standard tile sizes work.
This suggests that some empirical testing of different tile sizes might be necessary. Long story short. 256 tile size does very well for normal residential buildings but not for larger commercial buildings. So, for our case where we have  wide distribution of sizes the max of which is 380 pixels, we use 400 x 400 tile size. This enables us to fully enclose <strong><em>most</em></strong> buildings in our image chips. Since the majority of buildings are residential houses this should work well.If the opposite were true with most being large commercial buildings or warehouses we would need to use 512 or larger.</p>
<p><img src= "../images/TrainingChip_Image.png" alt ="Parameters for exporting samples for training a Deep Learning model" style= " width 600px; height: 400px;">
<img src= "../images/TrainingChip_Label" alt ="Parameters for exporting samples for training a Deep Learning model" style= " width 600px; height: 400px;"></p>
<p>Next we choose a stride size which indicates the distance to move in pixels for each new image chip, thereby controlling overlap between images. Here we will follow a rule of thumb based on prior examples using stride size of 25% the tile size. That works out to 64 pixels.
Since, we are using an RCNN Mask DL model so we select the metadata format RCNN Masks. The metadata stores each building object's segmentation mask (pixel accurate object outline) as polygons within the image and tells the model how to interpret object locations in each tile. The map space parameter is important for converting the pixel-based segmentation masks on the image to 
georeferenced vector data, enabling the model to map masked objects to their geographical locations
Then we run the tool to export these training samples.</p>
<h4><strong><em>Custom Building Footprint Extraction Model Inference Result:</em></strong></h4>
<p>We now have created a DL model that is fine-tuned to our data. Here we call this model : Dominica_10cm_BuildingFootprint_model. 
Use the 'Detect Objects Using Deep Learning' tool in ArcGIS Pro. We will input our image, select the new fine-tuned DL model, and assign name for the output building footprint feature class. Here is a view of the tool and settings used.</p>
<p><img src= "../images/ArcGIS_Pro_DetectObjectUsing_DeepLearning_Tool.png" alt ="Parameters for exporting samples for training a Deep Learning model" style= " width 600px; height: 600px;"></p>
<p>Now, let's see how well the custom model detects buildings.</p>
<p><img src= "../images/BuildingExtraction_results.png" alt ="Parameters for exporting samples for training a Deep Learning model" style= " width 600px; height: 600px;"></p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>counts</th>
<th>% Detected</th>
</tr>
</thead>
<tbody>
<tr>
<td>pt-DL otb Model</td>
<td>253</td>
<td>82%</td>
</tr>
<tr>
<td>fined-tuned pt-DL Model</td>
<td>298</td>
<td>97%</td>
</tr>
<tr>
<td>Ground truth Buildings</td>
<td>307</td>
<td>100%</td>
</tr>
</tbody>
</table>
<p>The results show that training the out-of-the box USA Building footprint model improved the results. 298 of the 307 buildings that were digitized manually were successfully detected after training the model.
Key to this improvement was the model training on samples from this new geography and different resolution. However, there still are issues with false positives, such as detection of a pool but this was limited to one instance in this imagery. It could be that in an image with many large pools the model may have more false positives. The model also did not detect some of the objects that were smaller than residential buildings (e.g., sheds).
Altogether, training the USA building footprint model with our imagery improved its capability with minimal negative impact on false positives.</p>
<p>Sources: <code>^1</code> DominicaNewOnlines 2018. https://dominicanewsonline.com/news/undp/undp-news/building-damage-assessment-in-dominica-bda/
         <code>^2</code> Improve a deep learning model with transfer learning 2024. https://learn.arcgis.com/en/projects/improve-a-deep-learning-model-with-transfer-learning/</p>                </article>
            </aside><!-- /#featured -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="https://www.spc.noaa.gov/products/outlook/day1otlk.html">NOAA Storm Prediction Center</a></li>
                            <li><a href="https://www.weather.gov/">Active Weather Hazards</a></li>
                            <li><a href="https://github.com/estevezb">My Github Page</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://x.com/bestevez100">Twitter/X</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>